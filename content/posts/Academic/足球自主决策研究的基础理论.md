---
title: "足球自主决策研究的基础理论"
date: "2025-08-05T11:12:42+08:00"
categories: "Academic"
tags: ["Academic"]
author: "Baofeng Zhang"
showToc: true
TocOpen: false
draft: false
hidemeta: false
comments: false
description: "Desc Text."
canonicalURL: "https://canonical.url/to/page"
disableHLJS: true # to disable highlightjs
disableShare: false
disableHLJS: false
hideSummary: false
searchHidden: false
ShowReadingTime: true
ShowBreadCrumbs: true
ShowPostNavLinks: true
ShowWordCount: true
ShowRssButtonInSectionTermList: true
UseHugoToc: true
cover:
    image: "<image path/url>"
    alt: "<alt text>" 
    caption: "<text>" # display caption under cover
    relative: false # when using page bundles set this to true
    hidden: true # only hide on current single page
editPost:
    URL: "https://github.com/<path_to_repo>/content"
    Text: "Suggest Changes" # edit text
    appendFilePath: true # to append file path to Edit link
---

## Supervised Imitation Learning (Ross et al., 2011, AISTATS)

[A Reduction of Imitation Learning and Structured Prediction to No-Regret Online Learning](https://proceedings.mlr.press/v15/ross11a)

Supervised Imitation Learning, as described in the document, is a traditional approach where a policy is trained to mimic an expert's behavior by directly applying standard supervised learning algorithms to a dataset of expert demonstrations; specifically, it minimizes a surrogate loss function, such as 0-1 loss or squared loss, under the distribution of states encountered by the expert during these demonstrations. This method involves collecting trajectories of states and actions from the expert, then training a classifier or regressor to predict the expert's actions given the observed states, ignoring the fact that the learner's own policy will alter the state distribution during execution, thereby violating the i.i.d. assumption crucial for statistical learning. Consequently, while the trained policy may achieve low error rates under the expert's state distribution, it suffers from poor generalization under its induced distribution, as initial mistakes can compound over time steps, leading to a quadratic increase in expected cost (e.g., errors growing as T2ϵ for horizon T and error ϵ), making it inferior to iterative approaches like DAGGER that account for distributional shift.

## Generative Adversarial Imitation Learning (Song et al., 2018, NeurIPS)

[Multi-Agent Generative Adversarial Imitation Learning](https://proceedings.neurips.cc/paper/2018/hash/240c945bb72980130446fc2b40fbb8e0-Abstract.html)

Multi-Agent Generative Adversarial Imitation Learning (MAGAIL) is a framework designed for imitation learning in multi-agent settings, extending the single-agent Generative Adversarial Imitation Learning (GAIL) to handle environments with multiple interacting agents, such as cooperative or competitive scenarios. It operates through an adversarial training process: a generator, which learns the joint policies of all agents to mimic expert behaviors, competes against discriminators—one for each agent—that aim to distinguish between trajectories generated by the agents and those from expert demonstrations. The optimization objective minimizes the generator's loss while maximizing the discriminators' ability to classify behaviors, formalized as min_θ max_ω E_π[∑_i log D_ω_i(s, a_i)] + E_π_E[∑_i log(1 - D_ω_i(s, a_i))], where θ parameterizes the policies and ω the discriminators. MAGAIL adapts to different prior knowledge structures, including centralized (shared rewards), decentralized (independent rewards per agent), and zero-sum (opposing rewards) configurations, enabling it to imitate complex high-dimensional interactions without access to the true rewards, as validated in experiments on particle environments and cooperative control tasks. This approach effectively addresses challenges like non-stationarity and multiple equilibria, achieving performance close to expert levels by leveraging multi-agent reinforcement learning techniques for stable policy updates.

## Learning from Demonstrations (Hester et al., 2018, AAAI)

[Deep Q-learning From Demonstrations](https://ojs.aaai.org/index.php/AAAI/article/view/11757)

Deep Q-learning from Demonstrations (DQfD) is a novel reinforcement learning algorithm designed to address the challenge of poor initial performance and high data requirements in deep RL, particularly for real-world tasks where simulators are unavailable. It leverages a small set of demonstration data from previous controllers (e.g., human players) to accelerate learning by pre-training the agent using a combination of losses: temporal difference (TD) updates for self-consistent value functions, a supervised large-margin classification loss to imitate demonstrator actions, n-step returns for better value propagation, and L2 regularization to prevent overfitting. After pre-training, the agent interacts with the environment, mixing demonstration and self-generated data via a prioritized replay mechanism that automatically balances their sampling ratio to optimize learning. This approach significantly boosts early performance, outperforming baseline methods like Prioritized Dueling Double DQN (PDD DQN) in 41 out of 42 Atari games during the first million steps, achieving state-of-the-art results in challenging exploration games and enabling the agent to exceed demonstrator performance in several cases.

## Inverse Reinforcement Learning (Bergerson, 2021, arXiv)

[Multi-Agent Inverse Reinforcement Learning: Suboptimal Demonstrations and Alternative Solution Concepts](https://arxiv.org/abs/2109.01178)

Multi-Agent Inverse Reinforcement Learning (MIRL) extends the core concept of single-agent Inverse Reinforcement Learning (IRL) to environments with multiple interacting agents. Its primary goal is to infer the underlying reward functions that explain the observed behaviors of these agents. Unlike single-agent IRL, MIRL must explicitly account for the complex interdependencies arising from agent interactions, such as competition, cooperation, or mixed motives. This involves reasoning about how each agent's actions are influenced by and influence the behaviors of others. Consequently, MIRL techniques often rely on multi-agent solution concepts (like Nash equilibrium or its generalizations) to model realistic social dynamics and handle challenges like suboptimal demonstrations, noise, and strategic reasoning inherent in multi-agent systems.



## Adaptive Action Supervision (Fujii et al., 2024, ICAART)

[Adaptive action supervision in reinforcement learning from real-world multi-agent demonstrations](https://arxiv.org/abs/2305.13030)

Adaptive Action Supervision is a novel method in reinforcement learning (RL) designed to bridge the domain gap between real-world multi-agent demonstrations and simulated RL environments, particularly when the source environment dynamics are unknown. It integrates RL with supervised learning by adaptively selecting actions from demonstrations based on the minimum distance calculated via Dynamic Time Warping (DTW), which aligns trajectories across domains to account for timing discrepancies and environmental differences. This approach enhances the balance between reproducibility—by closely imitating expert behaviors—and generalization—by enabling agents to optimize rewards in target environments, such as in chase-and-escape scenarios or football tasks using professional player data. Experiments demonstrate its effectiveness in maintaining this balance compared to baselines, making it a versatile solution for applications like sports analytics and biological modeling without requiring explicit source dynamics knowledge.



------



## Fictitious Self-Play (Heinrich et al., 2016, arXiv)

[Deep Reinforcement Learning from Self-Play in Imperfect-Information Games](https://arxiv.org/abs/1603.01121)

Neural Fictitious Self-Play (NFSP) introduces the first scalable end-to-end deep reinforcement learning approach for learning approximate Nash equilibria in imperfect-information games without prior domain knowledge. This method combines Fictitious Self-Play (FSP) with neural network function approximation, where each agent utilizes two neural networks: one trained via reinforcement learning to approximate best responses against opponents' historical strategies, and another trained via supervised learning to model the agent's own average behavioral strategy. Agents act using a mixture of these strategies, stabilized by reservoir sampling for experience storage and anticipatory dynamics for simultaneous learning convergence.



## Policy-Space Response Oracles (Lanctot et al., 2017, NeurIPS)

[A Unified Game-Theoretic Approach to Multiagent Reinforcement Learning](https://proceedings.neurips.cc/paper/2017/hash/3323fe11e9595c09af38fe67567a9394-Abstract.html)

Policy-Space Response Oracles (PSRO) is a generalized MARL algorithm that extends concepts like Double Oracle and Fictitious Self-Play by operating directly in policy spaces rather than action spaces. It iteratively grows a population of policies for each agent through epochs where new "oracle" policies—approximate best responses computed via deep reinforcement learning—are added to counter the current mixture of opponents' strategies (meta-strategies). Empirical game-theoretic analysis is then used to update meta-strategy distributions (e.g., using regret-matching, Hedge, or projected replicator dynamics) based on simulated payoffs across joint policy combinations. This approach mitigates the overfitting problem inherent in independent reinforcement learning, as quantified by reduced joint-policy correlation, by training policies against diverse strategy mixtures rather than fixed opponents, thereby producing more robust and generalizable behaviors in partially observable environments like coordination games and imperfect-information settings such as poker.

## League-Based Training (Vinyals et al., 2019, Nature)

[Grandmaster level in StarCraft II using multi-agent reinforcement learning](https://www.nature.com/articles/s41586-019-1724-z)

League Training is a MARL framework designed to address the non-transitive strategy cycles. It employs three agent pools—**main agents** that adaptively focus training on the most challenging opponents via *Prioritized Fictitious Self-Play (PFSP)*, **main exploiters** that target weaknesses in current strategies, and **league exploiters** that discover global blind spots across the entire league—all while periodically resetting exploiters to human-supervised policies to inject diversity and prevent overfitting. This structure creates a continuous "arms race" of counter-strategies, enabling emergent tactical complexity and robustness, ultimately allowing AlphaStar to achieve Grandmaster-level performance validated by professional players under human-like constraints.









